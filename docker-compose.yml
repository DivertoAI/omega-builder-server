services:
  omega:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: omega-builder
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      PYTHONUNBUFFERED: "1"
      PYTHONPATH: /app
      OMEGA_RESET_ON_START: "1"

      # --- Models: stick to o3 (planner) + gpt-5 (coder) ---
      OMEGA_PLANNER_MODEL: "o3"
      OMEGA_CODEGEN_MODEL: "gpt-5"
      OMEGA_LLM_MODEL: "gpt-5"
      OMEGA_IMAGE_MODEL: "gpt-image-1"

      # Point the app to the Redis service (compose DNS name is 'redis')
      REDIS_URL: "redis://redis:6379/0"

      # ==== AI-VM wiring (remote compile/repair) ====
      OMEGA_COMPILE_REMOTE: "1"           # prefer AI-VM for compile/test/repair
      OMEGA_COMPILE_LOCAL_FALLBACK: "1"   # if remote fails, fall back to local
      AI_VM_URL: "http://ai-vm:8080"      # service DNS inside compose network

      # Ensure omega writes artifacts where ai-vm can see them
      OMEGA_STAGING_ROOT: "/workspace/staging"

      # Queues (build + assets) — must match ai-vm
      AI_VM_QUEUE_KEY: "queue:build"
      AI_VM_QUEUE_ASSETS: "queue:assets"

      # Optional safety/time caps
      OMEGA_MAX_AGENT_ROUNDS: "8"
      OMEGA_DEFAULT_WALL_CLOCK_SEC: "900"
    volumes:
      - .:/app:delegated
      - .:/workspace:delegated          # shared path visible to both omega and ai-vm
      - omega_workspace:/app/workspace/.omega
    command: >
      python -m uvicorn backend.main:app
      --host 0.0.0.0
      --port 8000
    tty: true
    stdin_open: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/api/health"]
      interval: 2m
      timeout: 5s
      retries: 1
      start_period: 30s
    depends_on:
      redis:
        condition: service_healthy
      ai-vm:
        condition: service_healthy

  ai-vm:
    build:
      context: ./ai-vm
      dockerfile: docker/Dockerfile
    container_name: omega-ai-vm
    ports:
      - "8080:8080"
    environment:
      # where the VM expects to see the repo/workspace
      WORKSPACE_DIR: "/workspace"
      # flutter version can be overridden at build arg time too
      FLUTTER_VERSION: "3.27.1"
      # reduce tool noise in CI-like runs
      CI: "true"
      REDIS_URL: "redis://redis:6379/0"

      # Queues (build + assets) — must match omega
      AI_VM_QUEUE_KEY: "queue:build"
      AI_VM_QUEUE_ASSETS: "queue:assets"

      # Image model for assets worker
      OMEGA_IMAGE_MODEL: "gpt-image-1"
    volumes:
      # Mount the repo at /workspace so the VM can act on the generated app
      - .:/workspace:delegated
      # Cache pub packages & Gradle between runs (improves speed)
      - ai_vm_pub_cache:/home/flutter/.pub-cache
      - ai_vm_gradle_cache:/home/flutter/.gradle
    command: >
      bash -lc '
        # start the API
        uvicorn app.main:app --host 0.0.0.0 --port 8080 &
        # start the assets worker (logs to stdout)
        exec python -u workers/assets_worker.py
      '
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/api/health"]
      interval: 2m
      timeout: 8s
      retries: 1
      start_period: 30s
    depends_on:
      redis:
        condition: service_healthy

  redis:
    image: redis:7-alpine
    container_name: omega-redis
    command: ["redis-server", "--appendonly", "yes"]
    healthcheck:
      test: ["CMD", "redis-cli", "PING"]
      interval: 10s
      timeout: 3s
      retries: 10
    restart: unless-stopped
    # Optional: expose on host for debugging
    # ports:
    #   - "6379:6379"

volumes:
  omega_workspace:
  ai_vm_pub_cache:
  ai_vm_gradle_cache: